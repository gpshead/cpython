# binascii Base64 Benchmark

This directory contains a benchmark tool for measuring the performance of
the `binascii` module's base64 encoding and decoding functions.

## Usage

```bash
# Quick benchmark with default sizes
python Tools/binasciibench/binasciibench.py --quick

# Full benchmark with all sizes
python Tools/binasciibench/binasciibench.py

# Custom sizes and iterations
python Tools/binasciibench/binasciibench.py --sizes 64,1024,65536 --iterations 10

# Scaling analysis across many sizes
python Tools/binasciibench/binasciibench.py --scaling
```

## Vectorization Optimization

The base64 encoding/decoding in `Modules/binascii.c` has been optimized by
restructuring the loops to eliminate loop-carried dependencies, enabling
better compiler optimization.

### The Problem with the Original Code

The original encoding loop accumulated state across iterations:

```c
// Original code - has loop-carried dependencies
for (; bin_len > 0; bin_len--, bin_data++) {
    leftchar = (leftchar << 8) | *bin_data;  // Depends on previous iteration
    leftbits += 8;                            // Depends on previous iteration
    while (leftbits >= 6) {
        this_ch = (leftchar >> (leftbits-6)) & 0x3f;
        leftbits -= 6;
        *ascii_data++ = table_b2a_base64[this_ch];
    }
}
```

This pattern prevents the compiler from:
1. Unrolling the loop effectively
2. Reordering memory operations
3. Using instruction-level parallelism

### The Optimized Approach

The new code processes complete 3-byte groups (which produce exactly 4 base64
characters) without any loop-carried state:

```c
// Optimized code - each iteration is independent
static inline void
base64_encode_trio(const unsigned char *in, unsigned char *out,
                   const unsigned char *table)
{
    // Combine 3 bytes into a 24-bit value
    unsigned int combined = ((unsigned int)in[0] << 16) |
                            ((unsigned int)in[1] << 8) |
                            (unsigned int)in[2];

    // Extract four 6-bit groups - all independent operations
    out[0] = table[(combined >> 18) & 0x3f];
    out[1] = table[(combined >> 12) & 0x3f];
    out[2] = table[(combined >> 6) & 0x3f];
    out[3] = table[combined & 0x3f];
}

// Main loop - each iteration processes one complete group
for (i = 0; i < n_trios; i++) {
    base64_encode_trio(in + i * 3, out + i * 4, table);
}
```

### Why This Is Faster

1. **No Loop-Carried Dependencies**: Each iteration is completely independent,
   allowing the CPU to execute multiple iterations in parallel via
   instruction-level parallelism (ILP).

2. **Better Memory Access Pattern**: The compiler can see that we read 3 bytes
   and write 4 bytes per iteration, enabling better prefetching and pipelining.

3. **Table Lookups Remain Fast**: We keep the 64-byte lookup table (fits in L1
   cache) rather than trying arithmetic conversion which has unpredictable
   branches.

4. **Predictable Branch Behavior**: The main loop has a single predictable
   branch (the loop condition), unlike the original's inner while loop.

## Performance Results

Measured on x86_64 with GCC 13.3.0:

| Operation       | Before (MB/s) | After (GB/s) | Speedup |
|-----------------|---------------|--------------|---------|
| b2a_base64 64K  | 222           | 1.83         | 8.3x    |
| a2b_base64 64K  | 424           | 1.20         | 2.8x    |

## Annotated Assembly Analysis

The following assembly was generated by GCC 13.3.0 on x86_64 with
`-O3 -march=native` flags.

### Encoding Hot Loop (base64_encode_trio inlined)

```asm
# Architecture: x86_64
# Compiler: GCC 13.3.0
# Flags: -O3 -march=native

# Loop processes one 3-byte group per iteration
# Register allocation:
#   %rdi = input pointer (incremented by 3 each iteration)
#   %rcx = loop counter (0, 1, 2, ...)
#   %r8  = output pointer base
#   %r9  = table_b2a_base64 pointer

    3bd0:   0f b6 07             movzbl (%rdi),%eax
            # Load first input byte into %eax

    3bd3:   0f b6 77 01          movzbl 0x1(%rdi),%esi
            # Load second input byte into %esi

    3bd7:   48 83 c7 03          add    $0x3,%rdi
            # Advance input pointer by 3 bytes (for next iteration)

    3bdb:   c1 e6 08             shl    $0x8,%esi
            # Shift second byte left by 8 bits

    3bde:   c1 e0 10             shl    $0x10,%eax
            # Shift first byte left by 16 bits

    3be1:   09 f0                or     %esi,%eax
            # Combine: (byte0 << 16) | (byte1 << 8)

    3be3:   0f b6 77 ff          movzbl -0x1(%rdi),%esi
            # Load third input byte (using updated pointer - 1)

    3be7:   41 89 c2             mov    %eax,%r10d
            # Copy combined value to %r10d for first lookup

    3bea:   41 c1 ea 12          shr    $0x12,%r10d
            # Shift right 18 bits to get first 6-bit index

    3bee:   09 c6                or     %eax,%esi
            # Combine all three bytes: (byte0<<16)|(byte1<<8)|byte2

    3bf0:   c1 e8 0c             shr    $0xc,%eax
            # Shift right 12 bits for second 6-bit index

    3bf3:   47 0f b6 14 11       movzbl (%r9,%r10,1),%r10d
            # Table lookup for first output character

    3bf8:   83 e0 3f             and    $0x3f,%eax
            # Mask to 6 bits (second index)

    3bfb:   45 88 14 88          mov    %r10b,(%r8,%rcx,4)
            # Store first output character

    3bff:   41 0f b6 04 01       movzbl (%r9,%rax,1),%eax
            # Table lookup for second output character

    3c04:   41 88 44 88 01       mov    %al,0x1(%r8,%rcx,4)
            # Store second output character

    3c09:   89 f0                mov    %esi,%eax
            # Copy combined value for third/fourth lookups

    3c0b:   83 e6 3f             and    $0x3f,%esi
            # Mask to 6 bits (fourth index)

    3c0e:   c1 e8 06             shr    $0x6,%eax
            # Shift right 6 bits for third index

    3c11:   83 e0 3f             and    $0x3f,%eax
            # Mask to 6 bits (third index)

    3c14:   41 0f b6 04 01       movzbl (%r9,%rax,1),%eax
            # Table lookup for third output character

    3c19:   41 88 44 88 02       mov    %al,0x2(%r8,%rcx,4)
            # Store third output character

    3c1e:   41 0f b6 04 31       movzbl (%r9,%rsi,1),%eax
            # Table lookup for fourth output character

    3c23:   41 88 44 88 03       mov    %al,0x3(%r8,%rcx,4)
            # Store fourth output character

    3c28:   48 ff c1             inc    %rcx
            # Increment loop counter

    3c2b:   48 39 ca             cmp    %rcx,%rdx
            # Compare counter with number of trios

    3c2e:   7f a0                jg     3bd0
            # Jump back if more trios to process
```

### Decoding Hot Loop (base64_decode_quad inlined)

```asm
# The decode loop uses a word-at-a-time technique for padding detection.
# Instead of 4 separate byte comparisons, it loads all 4 bytes as a 32-bit
# word and uses arithmetic to check if any byte equals '=' (0x3d).

    477e:   41 8b 54 b5 00       mov    0x0(%r13,%rsi,4),%edx
            # Single 32-bit load of all 4 input bytes at once

    4783:   4c 8d 34 b5 00 00 00 lea    0x0(,%rsi,4),%r14
            # Calculate byte offset for this quad

    478b:   89 d0                mov    %edx,%eax
            # Copy for padding check

    478d:   89 d1                mov    %edx,%ecx
            # Copy for bitwise operations

    478f:   35 3d 3d 3d 3d       xor    $0x3d3d3d3d,%eax
            # XOR with '====' pattern - any '=' byte becomes 0x00

    4794:   81 f1 c2 c2 c2 c2    xor    $0xc2c2c2c2,%ecx
            # XOR with ~0x3d3d3d3d (bitwise NOT of pad pattern)

    479a:   2d 01 01 01 01       sub    $0x1010101,%eax
            # Subtract 1 from each byte position

    479f:   21 c8                and    %ecx,%eax
            # AND with inverted original

    47a1:   a9 80 80 80 80       test   $0x80808080,%eax
            # Test high bits - non-zero means a zero byte was found

    47a6:   75 37                jne    47df
            # Exit fast path if padding character found

    # Now extract individual bytes from the already-loaded 32-bit word
    # for table lookups (no redundant memory loads)
    47a8:   0f b6 c2             movzbl %dl,%eax
            # Extract first byte (low byte of quad)

    47ab:   41 0f b6 3c 00       movzbl (%r8,%rax,1),%edi
            # Table lookup: convert first char to 6-bit value

    47b0:   0f b6 c6             movzbl %dh,%eax
            # Extract second byte

    47b3:   41 0f b6 0c 00       movzbl (%r8,%rax,1),%ecx
            # Table lookup: convert second char

    47b8:   89 d0                mov    %edx,%eax
    47ba:   c1 ea 18             shr    $0x18,%edx
            # Shift right 24 bits to get fourth byte

    47bd:   45 0f b6 14 10       movzbl (%r8,%rdx,1),%r10d
            # Table lookup: convert fourth char

    47c2:   c1 e8 10             shr    $0x10,%eax
            # Shift right 16 bits to get third byte

    47c5:   0f b6 c0             movzbl %al,%eax
    47ca:   41 0f b6 04 00       movzbl (%r8,%rax,1),%eax
            # Table lookup: convert third char
```

The "has zero byte" technique works as follows:
1. XOR with the repeated target byte turns matches into 0x00
2. The expression `((x - 0x01010101) & ~x & 0x80808080)` is non-zero iff
   any byte in `x` is zero
3. This compiles to just 5 arithmetic ops instead of 4 loads + 4 compares

### Key Observations

1. **Instruction-Level Parallelism**: The compiler interleaves independent
   operations. While one table lookup is in progress, other operations
   (shifts, masks) can execute on different execution units.

2. **Memory Access Optimization**: The compiler pre-increments the input
   pointer (`add $0x3,%rdi`) early in the loop, allowing memory loads for
   the next iteration to begin while the current iteration is still
   processing.

3. **Efficient Output Pattern**: Output bytes are written using indexed
   addressing (`mov %al,0x1(%r8,%rcx,4)`), allowing the store buffer to
   coalesce adjacent writes.

4. **Word-at-a-Time Padding Detection**: The decode loop loads 4 bytes as
   a single 32-bit word and uses the "has zero byte" technique to check
   for padding characters. This replaces 4 separate loads and 4 separate
   comparisons with a single load and 5 arithmetic operations.

5. **Register Reuse**: The pre-loaded 32-bit word is reused for byte
   extraction via shifts and masks (`movzbl %dl`, `movzbl %dh`, `shr`),
   eliminating redundant memory accesses for table lookups.

## Building for Benchmarks

For accurate performance measurements, build Python with optimizations
but without PGO (which is slow):

```bash
# x86-64 with AVX2
mkdir build && cd build
CFLAGS="-O3 -march=x86-64-v3" ../configure
make -j

# ARM with NEON
mkdir build && cd build
CFLAGS="-O3 -march=armv8.2-a" ../configure
make -j
```

Do NOT use `--with-pydebug` as debug builds include assertions that
significantly skew performance measurements.
