# binascii Base64 Benchmark

This directory contains a benchmark tool for measuring the performance of
the `binascii` module's base64 encoding and decoding functions.

## Usage

```bash
# Quick benchmark with default sizes
python Tools/binasciibench/binasciibench.py --quick

# Full benchmark with all sizes
python Tools/binasciibench/binasciibench.py

# Custom sizes and iterations
python Tools/binasciibench/binasciibench.py --sizes 64,1024,65536 --iterations 10

# Scaling analysis across many sizes
python Tools/binasciibench/binasciibench.py --scaling
```

## Vectorization Optimization

The base64 encoding/decoding in `Modules/binascii.c` has been optimized by
restructuring the loops to eliminate loop-carried dependencies, enabling
better compiler optimization.

### The Problem with the Original Code

The original encoding loop accumulated state across iterations:

```c
// Original code - has loop-carried dependencies
for (; bin_len > 0; bin_len--, bin_data++) {
    leftchar = (leftchar << 8) | *bin_data;  // Depends on previous iteration
    leftbits += 8;                            // Depends on previous iteration
    while (leftbits >= 6) {
        this_ch = (leftchar >> (leftbits-6)) & 0x3f;
        leftbits -= 6;
        *ascii_data++ = table_b2a_base64[this_ch];
    }
}
```

This pattern prevents the compiler from:
1. Unrolling the loop effectively
2. Reordering memory operations
3. Using instruction-level parallelism

### The Optimized Approach

The new code processes complete 3-byte groups (which produce exactly 4 base64
characters) without any loop-carried state:

```c
// Optimized code - each iteration is independent
static inline void
base64_encode_trio(const unsigned char *in, unsigned char *out,
                   const unsigned char *table)
{
    // Combine 3 bytes into a 24-bit value
    unsigned int combined = ((unsigned int)in[0] << 16) |
                            ((unsigned int)in[1] << 8) |
                            (unsigned int)in[2];

    // Extract four 6-bit groups - all independent operations
    out[0] = table[(combined >> 18) & 0x3f];
    out[1] = table[(combined >> 12) & 0x3f];
    out[2] = table[(combined >> 6) & 0x3f];
    out[3] = table[combined & 0x3f];
}

// Main loop - each iteration processes one complete group
for (i = 0; i < n_trios; i++) {
    base64_encode_trio(in + i * 3, out + i * 4, table);
}
```

### Why This Is Faster

1. **No Loop-Carried Dependencies**: Each iteration is completely independent,
   allowing the CPU to execute multiple iterations in parallel via
   instruction-level parallelism (ILP).

2. **Better Memory Access Pattern**: The compiler can see that we read 3 bytes
   and write 4 bytes per iteration, enabling better prefetching and pipelining.

3. **Table Lookups Remain Fast**: We keep the 64-byte lookup table (fits in L1
   cache) rather than trying arithmetic conversion which has unpredictable
   branches.

4. **Predictable Branch Behavior**: The main loop has a single predictable
   branch (the loop condition), unlike the original's inner while loop.

## Performance Results

Measured on Intel Icelake-server (x86-64-v4 with AVX-512) with GCC 13.3.0,
`-O3 -march=native`:

### Scalar Optimization (loop restructuring)

| Operation       | Before        | After         | Speedup |
|-----------------|---------------|---------------|---------|
| b2a_base64 64K  | 1.10 GB/s     | 1.93 GB/s     | 1.75x   |
| a2b_base64 64K  | 295 MB/s      | 1.26 GB/s     | 4.27x   |

### AVX-512 VBMI SIMD (on CPUs with VBMI support)

CPUs with AVX-512 VBMI (Icelake, Zen4, etc.) get additional acceleration:

| Operation       | Scalar        | AVX-512 VBMI  | Speedup |
|-----------------|---------------|---------------|---------|
| b2a_base64 64K  | 1.93 GB/s     | 22.8 GB/s     | 11.8x   |
| a2b_base64 64K  | 1.26 GB/s     | 14.6 GB/s     | 11.6x   |

The SIMD implementation uses:
- `vpermb` for byte reshuffling across all 64 bytes
- `vpmaddubsw`/`vpmaddwd` for 6-bit packing/unpacking
- `vpermb` again for the final table lookup (encoding) or byte packing (decoding)

## Annotated Assembly Analysis

The following assembly was generated by GCC 13.3.0 on x86_64 with
`-O3 -march=native` (Icelake-server) flags.

### Encoding Hot Loop (base64_encode_trio inlined)

```asm
# Architecture: x86_64 (Intel Icelake-server, x86-64-v4)
# Compiler: GCC 13.3.0
# Flags: -O3 -march=native

# Loop processes one 3-byte group per iteration
# Register allocation:
#   %rdi = input pointer (incremented by 3 each iteration)
#   %rcx = loop counter (0, 1, 2, ...)
#   %r8  = output pointer base
#   %r9  = table_b2a_base64 pointer

    3bd0:   0f b6 07             movzbl (%rdi),%eax
            # Load first input byte into %eax

    3bd3:   0f b6 77 01          movzbl 0x1(%rdi),%esi
            # Load second input byte into %esi

    3bd7:   48 83 c7 03          add    $0x3,%rdi
            # Advance input pointer by 3 bytes (for next iteration)

    3bdb:   c1 e6 08             shl    $0x8,%esi
            # Shift second byte left by 8 bits

    3bde:   c1 e0 10             shl    $0x10,%eax
            # Shift first byte left by 16 bits

    3be1:   09 f0                or     %esi,%eax
            # Combine: (byte0 << 16) | (byte1 << 8)

    3be3:   0f b6 77 ff          movzbl -0x1(%rdi),%esi
            # Load third input byte (using updated pointer - 1)

    3be7:   41 89 c2             mov    %eax,%r10d
            # Copy combined value to %r10d for first lookup

    3bea:   41 c1 ea 12          shr    $0x12,%r10d
            # Shift right 18 bits to get first 6-bit index

    3bee:   09 c6                or     %eax,%esi
            # Combine all three bytes: (byte0<<16)|(byte1<<8)|byte2

    3bf0:   c1 e8 0c             shr    $0xc,%eax
            # Shift right 12 bits for second 6-bit index

    3bf3:   47 0f b6 14 11       movzbl (%r9,%r10,1),%r10d
            # Table lookup for first output character

    3bf8:   83 e0 3f             and    $0x3f,%eax
            # Mask to 6 bits (second index)

    3bfb:   45 88 14 88          mov    %r10b,(%r8,%rcx,4)
            # Store first output character

    3bff:   41 0f b6 04 01       movzbl (%r9,%rax,1),%eax
            # Table lookup for second output character

    3c04:   41 88 44 88 01       mov    %al,0x1(%r8,%rcx,4)
            # Store second output character

    3c09:   89 f0                mov    %esi,%eax
            # Copy combined value for third/fourth lookups

    3c0b:   83 e6 3f             and    $0x3f,%esi
            # Mask to 6 bits (fourth index)

    3c0e:   c1 e8 06             shr    $0x6,%eax
            # Shift right 6 bits for third index

    3c11:   83 e0 3f             and    $0x3f,%eax
            # Mask to 6 bits (third index)

    3c14:   41 0f b6 04 01       movzbl (%r9,%rax,1),%eax
            # Table lookup for third output character

    3c19:   41 88 44 88 02       mov    %al,0x2(%r8,%rcx,4)
            # Store third output character

    3c1e:   41 0f b6 04 31       movzbl (%r9,%rsi,1),%eax
            # Table lookup for fourth output character

    3c23:   41 88 44 88 03       mov    %al,0x3(%r8,%rcx,4)
            # Store fourth output character

    3c28:   48 ff c1             inc    %rcx
            # Increment loop counter

    3c2b:   48 39 ca             cmp    %rcx,%rdx
            # Compare counter with number of trios

    3c2e:   7f a0                jg     3bd0
            # Jump back if more trios to process
```

### Decoding Hot Loop (base64_decode_quad inlined)

```asm
# The decode loop checks for padding ('=') before processing

    4776:   0f b6 14 83          movzbl (%rbx,%rax,4),%edx
            # Load first input character

    477a:   4c 8d 24 85 00 00 00 lea    0x0(,%rax,4),%r12
            # Calculate byte offset for this quad

    4782:   80 fa 3d             cmp    $0x3d,%dl
            # Check if first char is '=' (padding)

    4785:   74 48                je     47cf
            # Exit fast path if padding found

    4787:   0f b6 4c 83 01       movzbl 0x1(%rbx,%rax,4),%ecx
            # Load second input character

    478c:   80 f9 3d             cmp    $0x3d,%cl
            # Check for padding

    478f:   74 3e                je     47cf

    4791:   44 0f b6 5c 83 02    movzbl 0x2(%rbx,%rax,4),%r11d
            # Load third input character

    4797:   41 80 fb 3d          cmp    $0x3d,%r11b
            # Check for padding

    479b:   74 32                je     47cf

    479d:   0f b6 7c 83 03       movzbl 0x3(%rbx,%rax,4),%edi
            # Load fourth input character

    47a2:   40 80 ff 3d          cmp    $0x3d,%dil
            # Check for padding

    47a6:   74 27                je     47cf

    # All four characters loaded, now do table lookups
    47a8:   41 0f b6 34 10       movzbl (%r8,%rdx,1),%esi
            # Table lookup: convert first char to 6-bit value

    47ad:   41 0f b6 0c 08       movzbl (%r8,%rcx,1),%ecx
            # Table lookup: convert second char

    47b2:   43 0f b6 14 18       movzbl (%r8,%r11,1),%edx
            # Table lookup: convert third char

    47b7:   45 0f b6 1c 38       movzbl (%r8,%rdi,1),%r11d
            # Table lookup: convert fourth char

    47bc:   89 f7                mov    %esi,%edi
    47be:   09 cf                or     %ecx,%edi
            # Combine values to check for invalid chars (values >= 64)
            # If any lookup returned 0xff (-1), the high bits will be set
```

### Key Observations

1. **Instruction-Level Parallelism**: The compiler interleaves independent
   operations. While one table lookup is in progress, other operations
   (shifts, masks) can execute on different execution units.

2. **Memory Access Optimization**: The compiler pre-increments the input
   pointer (`add $0x3,%rdi`) early in the loop, allowing memory loads for
   the next iteration to begin while the current iteration is still
   processing.

3. **Efficient Output Pattern**: Output bytes are written using indexed
   addressing (`mov %al,0x1(%r8,%rcx,4)`), allowing the store buffer to
   coalesce adjacent writes.

4. **Branch-Free Processing**: The main processing has no conditional
   branches. The only branch is the loop condition, which is highly
   predictable. (Decoding has additional padding checks, but these are
   separate from the core arithmetic.)

## Building for Benchmarks

For accurate performance measurements, build Python with optimizations
but without PGO (which is slow):

```bash
# x86-64 with AVX2
mkdir build && cd build
CFLAGS="-O3 -march=x86-64-v3" ../configure
make -j

# ARM with NEON
mkdir build && cd build
CFLAGS="-O3 -march=armv8.2-a" ../configure
make -j
```

Do NOT use `--with-pydebug` as debug builds include assertions that
significantly skew performance measurements.
